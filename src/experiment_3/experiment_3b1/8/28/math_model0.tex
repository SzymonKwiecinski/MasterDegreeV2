\documentclass{article}
\usepackage{amsmath}
\begin{document}

\section*{Linear Programming Model for Fitting a Line to Data}

Given a set of data points \((x_k, y_k)\) for \(k = 1, 2, \ldots, K\), we aim to find parameters \(a\) (intercept) and \(b\) (slope) that minimize the sum of absolute deviations of the observed values of \(y\) from the predicted values given by the linear equation:

\[
y = bx + a
\]

\subsection*{Variables}
Let \(d_k\) represent the absolute deviation for each data point:

\[
d_k = y_k - (bx_k + a)
\]

We then express the absolute deviation using auxiliary variables \(d_k^+\) and \(d_k^-\):

\[
d_k^+ \geq d_k \quad \text{and} \quad d_k^- \geq -d_k
\]

Thus we can write:

\[
d_k = d_k^+ - d_k^-
\]

The objective is to minimize the total absolute deviation:

\[
\text{Minimize} \quad Z = \sum_{k=1}^{K} (d_k^+ + d_k^-)
\]

\subsection*{Subject to Constraints}
The following constraints need to be formulated:

\[
y_k - (bx_k + a) \leq d_k^+ \quad \text{for } k = 1, 2, \ldots, K
\]

\[
-(y_k - (bx_k + a)) \leq d_k^- \quad \text{for } k = 1, 2, \ldots, K
\]

\subsection*{Final Model}
The complete Linear Programming model can thus be stated as:

\[
\begin{align*}
\text{Minimize} \quad & Z = \sum_{k=1}^{K} (d_k^+ + d_k^-) \\
\text{subject to} \quad & y_k - (bx_k + a) \leq d_k^+ \quad \forall k \\
& -(y_k - (bx_k + a)) \leq d_k^- \quad \forall k \\
& d_k^+, d_k^- \geq 0 \quad \forall k
\end{align*}
\]

\subsection*{Output}
The solution will yield values for \(a\) (intercept) and \(b\) (slope) defined as:

\[
\{ "intercept": a, "slope": b \}
\]

\end{document}