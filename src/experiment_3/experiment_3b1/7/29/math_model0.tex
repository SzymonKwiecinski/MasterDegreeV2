\documentclass{article}
\usepackage{amsmath}
\begin{document}

\section*{Linear Programming Model for Best-Fit Line}

Let us define our model for fitting the best straight line \( y = bx + a \) to the given datasets \((x_k, y_k)\) for \( k = 1, 2, \ldots, K \).

\subsection*{Variables}
\begin{align*}
a & \quad \text{(intercept)} \\
b & \quad \text{(slope)} \\
d_k & \quad \text{(deviation for each observation)} \quad \text{for } k = 1, \ldots, K
\end{align*}

\subsection*{Objective}
We want to minimize the maximum deviation between the observed values \( y_k \) and the predicted values \( \hat{y}_k = bx_k + a \). Our objective can be formulated as:
\[
\text{Minimize } D
\]
where
\[
D \geq |y_k - (bx_k + a)| \quad \text{for } k = 1, 2, \ldots, K
\]

\subsection*{Constraints}
The constraints for our model can be expressed as:
\[
y_k - (bx_k + a) \leq D \quad \text{for } k = 1, \ldots, K
\]
\[
-(y_k - (bx_k + a)) \leq D \quad \text{for } k = 1, \ldots, K
\]

\subsection*{Complete Model}
Putting everything together, the complete linear programming model is:

\[
\text{Minimize } D
\]

Subject to:
\[
y_k - (bx_k + a) \leq D, \quad k = 1, 2, \ldots, K
\]
\[
-(y_k - (bx_k + a)) \leq D, \quad k = 1, 2, \ldots, K
\]

\subsection*{Output Variables}
After solving the above linear programming model, the output will be:
\[
\text{Output: } 
\begin{cases}
\text{intercept} = a \\
\text{slope} = b
\end{cases}
\]

\end{document}